{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3c1ddc",
   "metadata": {},
   "source": [
    "# GenAI, RAG, Agents, Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba921a2",
   "metadata": {},
   "source": [
    "### 1: What is an LLM :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32afb85",
   "metadata": {},
   "source": [
    "* LLM ek large language model hota hai jo huge text data par train kiya gaya hota hai. Ye model natural language ko samajhta hai, generate karta hai, aur instructions follow karta hai.\n",
    "\n",
    "* LLM ko aise samajh ki ye ek intelligent text machine hai. Ye tumhare sawaal ko context ke saath samajh kar human jaise jawab banata hai.\n",
    "\n",
    "* Real world me text sabse common data hota hai, isliye LLMs bahut powerful tools ban gaye.\n",
    "\n",
    "* LLMs ko hum chatbots, summarization, code generation, translation, QnA, automation sab me use kar sakte hai.\n",
    "\n",
    "* Inka strength ye hota hai ki bina task specific training ke bhi bohot saare tasks samajh jaate hai.\n",
    "\n",
    "* LLMs reasoning, creativity aur context handling me best models ban chuke hai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a007c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Model name mention\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Tokenizer load\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Model load\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Explain what is a Large Language Model.\"\n",
    "\n",
    "# Prompt ko tokens me convert karna\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Output generate\n",
    "output_tokens = model.generate(\n",
    "    **inputs,\n",
    "    max_length=50,   # 50 tokens tak output\n",
    "    temperature=1.0  # randomness control\n",
    ")\n",
    "\n",
    "# Tokens ko text me decode karna\n",
    "print(tokenizer.decode(output_tokens[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13afb65",
   "metadata": {},
   "source": [
    "* Model ka naam.\n",
    "\n",
    "* Tokenizer load hota hai jo text ko numbers me convert karta hai.\n",
    "\n",
    "* Model load hota hai jo sequence generate karta hai.\n",
    "\n",
    "* Prompt wo input hai jo model ko guide karta hai.\n",
    "\n",
    "* Tokenization model ko data samajhne laayak banati hai.\n",
    "\n",
    "* Model generate karta hai next text based on input.\n",
    "\n",
    "* Decode karne se numbers wapas text me convert hote hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0133f485",
   "metadata": {},
   "source": [
    "### 2: Prompting :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4941c1",
   "metadata": {},
   "source": [
    "* Prompting matlab model ko correct instruction dena. Jaise tum human ko instructions dete ho, waise hi model ko bhi instructions diye jaate hai.\n",
    "\n",
    "* Prompting ek technique hai jisme hum model ko batate hai ki kya karna hai. Jitna clear prompt, utna better output.\n",
    "\n",
    "* LLM fully prompt driven model hai, agar prompt sahi hua to result meaningful aata hai.\n",
    "\n",
    "* Prompt engineering ek skill ban chuki hai jisse output quality improve hoti hai.\n",
    "\n",
    "* Isi se hum ek hi model se multiple tasks karwa sakte hai.\n",
    "\n",
    "* Bina fine tuning ke tasks chal jaate hai, sirf prompt se hi control ho jata hai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0049c310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful assistant.\n",
      "Explain tokenization in simple terms.\n",
      "Give a short example.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are a helpful assistant.\n",
    "Explain tokenization in simple terms.\n",
    "Give a short example.\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf62a0a",
   "metadata": {},
   "source": [
    "### 3: Temperature :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef51f1",
   "metadata": {},
   "source": [
    "* Temperature model ke randomness ko control karta hai.\n",
    "\n",
    "* Temperature ko tum creativity level samajh sakte ho.\n",
    "\n",
    "* Temperature low ho to model safe and accurate reply deta hai.\n",
    "\n",
    "* Temperature high ho to model more creative hota hai.\n",
    "\n",
    "* Controlled generation chahiye to use low temperature.\n",
    "\n",
    "* Creativity chahiye to high temperature.\n",
    "\n",
    "* Dialogue flow model ke temperature se decide hota hai.\n",
    "\n",
    "* Different tasks ke liye different temperature values best kaam karti hai.\n",
    "\n",
    "* 0 to 0.3 → predictable output\n",
    "\n",
    "* 0.7 → balanced\n",
    "\n",
    "* 1.0+ → creative output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ed74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tokens = model.generate(\n",
    "    **inputs,\n",
    "    max_length=60,\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc25f8c7",
   "metadata": {},
   "source": [
    "### 4: Tokens :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2116af",
   "metadata": {},
   "source": [
    "* Token small piece of text hota hai. Model text ko direct nahi samajhta, balki tokens me convert karke read karta hai.\n",
    "\n",
    "* Token words ka chhota version hota hai. Model har token ko number banata hai aur uske basis par next token predict karta hai.\n",
    "\n",
    "* Model ki cost tokens per hoti hai.\n",
    "\n",
    "* Token limit decide karta hai ki tum kitna long prompt de sakte ho.\n",
    "\n",
    "* Tokenization hi model ki input pipeline hai.\n",
    "\n",
    "* Text compression type ka system hai jo model ko efficient banata hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cb77c4",
   "metadata": {},
   "source": [
    "* Example :\n",
    "\n",
    "    - \"Hello world\" might be → [\"Hello\", \" world\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326e16e4",
   "metadata": {},
   "source": [
    "### 5: Use models via OpenRouter :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e21e6da",
   "metadata": {},
   "source": [
    "* OpenRouter ek API service hai jisme bahut sare models ek jagah milte hai jaise GPT, Claude, Llama, Mixtral.\n",
    "\n",
    "* Ye ek marketplace jaisa hai jaha tum ek hi API se multiple LLMs ko use kar sakte ho.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer YOUR_API_KEY\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/Llama-3-8b\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Explain LLM basics\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
