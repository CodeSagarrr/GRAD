{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f96faa",
   "metadata": {},
   "source": [
    "# LangChain basics :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3c975",
   "metadata": {},
   "source": [
    "* LangChain ek framework hai jo LLMs ke around applications banana easy banata hai.\n",
    "\n",
    "* Agar LLM engine hai, to LangChain us engine ka gearbox hai. Ye prompts, models, memory, tools sab ko connect karke ek proper AI app banane me help karta hai.\n",
    "\n",
    "* Why in AI ML : \n",
    "\n",
    "    - Single prompt se aage jaane ke liye LangChain zaroori hota hai.\n",
    "\n",
    "    - Multi step reasoning apps banana possible hota hai.\n",
    "\n",
    "    - Context and memory manage kar sakte hai.\n",
    "\n",
    "    - Real world AI apps isi type ke flow par bante hai.\n",
    "\n",
    "* Important subtopics :\n",
    "\n",
    "    - LLM wrapper : \n",
    "\n",
    "        - An LLM wrapper is a software layer that simplifies using a LLM by providing an easier interface, abstracting complex API calls into straightforward functions for tasks like text generation\n",
    "\n",
    "    - Prompt templates : \n",
    "    \n",
    "        - A Prompt Template is a structured, reusable recipe for generating a prompt to be sent to a Large Language Model (LLM).\n",
    "\n",
    "    - Chains :\n",
    "\n",
    "        - a Chain is a sequence of modular components (like LLMs, data retrievers, or custom logic) linked together to automate multi-step, context-aware tasks, passing the output of one step as the input to the next.\n",
    "\n",
    "    - Memory :\n",
    "\n",
    "        - Memory is the crucial component that lets Language Models (LLMs) remember past interactions, giving applications context for natural, continuous conversations, rather than treating each query as a fresh start. \n",
    "\n",
    "    - Tools :\n",
    "\n",
    "        - Tools are functions or APIs that extend an LLM's capabilities, allowing it to interact with the real world, access live data, perform calculations, or use other services beyond its static training, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a570b3e3",
   "metadata": {},
   "source": [
    "## Prompt Chains :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a73369c",
   "metadata": {},
   "source": [
    "* Prompt chain ka matlab hai multiple prompts ko ek flow me connect karna.\n",
    "\n",
    "* Ek hi sawaal ka jawab ek step me nahi milta. Isliye hum pehle ek prompt se output nikalte hai, fir us output ko next prompt ka input bana dete hai.\n",
    "\n",
    "* Why important :\n",
    "\n",
    "    - Complex problems ko steps me tod deta hai.\n",
    "\n",
    "    - Reasoning better hoti hai.\n",
    "\n",
    "    - Output zyada accurate aata hai.\n",
    "\n",
    "    - Real workflows possible hote hai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ebf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate, prompt\n",
    "from langchain_classic.chains import LLMChain\n",
    "\n",
    "\n",
    "model = GoogleGenerativeAI(api_key=\"AIzaSyAj_udih14a7Sw1nDz1v0pY_-maJRP8B3g\" ,temperature=0.7)\n",
    "\n",
    "# Prompt template create\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in simple words\"\n",
    ")\n",
    "\n",
    "# Chain create\n",
    "chain = LLMChain(\n",
    "    llm=model,\n",
    "    prompt= prompt\n",
    ")\n",
    "\n",
    "# Chain run\n",
    "response = chain.run(topic=\"javascript\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4126b6",
   "metadata": {},
   "source": [
    "* LLM initialize karte hai jisme temperature set hota hai.\n",
    "\n",
    "* PromptTemplate reusable prompt banata hai.\n",
    "\n",
    "* LLMChain prompt aur model ko connect karta hai.\n",
    "\n",
    "* run method se chain execute hoti hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3496fab",
   "metadata": {},
   "source": [
    "## Tools and Memory :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306144e9",
   "metadata": {},
   "source": [
    "* Tools external functions hote hai jise LLM call kar sakta hai.\n",
    "\n",
    "* LLM khud se calculation ya API call nahi karta. Tools ke through hum usse calculator, search ya custom function use karwa sakte hai.\n",
    "\n",
    "* Memory :\n",
    "\n",
    "    - Memory ka matlab hai conversation ka context save karna.\n",
    "\n",
    "    - Bina memory ke LLM har baar bhool jata hai.\n",
    "\n",
    "    - Memory se chatbot ko yaad rehta hai ki pehle kya baat hui thi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed961e",
   "metadata": {},
   "source": [
    "#### Example Tool :\n",
    "\n",
    "* Is function ko LangChain tool ke form me register kiya ja sakta hai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a16567d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a, b):\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f4fcbb",
   "metadata": {},
   "source": [
    "#### Example Memory :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffc299d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1h/wgc04mv53lb5ywfpf2lbj_bh0000gn/T/ipykernel_92031/174382074.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e3d75",
   "metadata": {},
   "source": [
    "* Why important :\n",
    "\n",
    "    - Chat like behavior aata hai.\n",
    "\n",
    "    - Long conversations handle hoti hai.\n",
    "\n",
    "    - User experience better hota hai.\n",
    "\n",
    "    - Real assistants memory use karte hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f98387",
   "metadata": {},
   "source": [
    "### first LLM app :\n",
    "\n",
    "* Ek simple chatbot banana jo memory use kare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42388e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_classic.chains import ConversationChain\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "# model load \n",
    "\n",
    "llm = GoogleGenerativeAI(api_key=\"AIzaSyAj_udih14a7Sw1nDz1v0pY_-maJRP8B3g\" , temperature=0.7)\n",
    "\n",
    "# Memory create\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Conversation chain\n",
    "\n",
    "chatbot = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Chat start\n",
    "print(chatbot.run(\"Hi, my name is Sagar\"))\n",
    "print(chatbot.run(\"What is my name?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122405c7",
   "metadata": {},
   "source": [
    "* OpenAI model LLM ka kaam karta hai.\n",
    "\n",
    "* Memory object previous messages store karta hai.\n",
    "\n",
    "* ConversationChain LLM aur memory ko connect karta hai.\n",
    "\n",
    "* Second question ka answer pehle context par based hota hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c44e0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
